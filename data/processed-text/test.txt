CS 331: Introduction to Artificial 
Intelligence
Lecture 2: Agents
Sandhya Saisubramanian

Announcements
‚Ä¢ Quiz 1 will be released on Oct 1 at 2 pm and will be active until Oct 3 
11:59pm
‚Ä¢ Topics for Quiz 1: Agents and uninformed search (Lectures 2,3)
‚Ä¢ HW1 will be posted on Oct 1; due on Oct 10
‚Ä¢ Change in TA office hours: Dilumika‚Äôs office hours will be on Friday 9-10 am 
(instead of Thursday)
2

Today‚Äôs Agenda
What is an agent?
What does it mean to design an intelligent agent?
3
How can these agents decide what to do/ how to act?
Readings: Chapters 2 and 3 in course textbook

Few Real-World Examples
At a high-level, all these systems perform the following:
1. Sense (perceive)
2. Think (decide what to do)
3. Act (produce some output)
4

Agent-Centric View of AI 
Reasoning
Environment
Percepts
Actions
Sensors
Actuators
Agent: anything that perceives its environment through
sensors and acts on that environment through actuators
Agent
5

Agent-Related Terms
‚Ä¢ Percept sequence (P): A complete history of everything the agent 
has ever perceived.  Think of this as the state of the world from 
the agent‚Äôs perspective.
‚Ä¢ Agent Policy: Maps percept sequence to action (determines agent 
behavior)
‚Ä¢ Also called as agent function in some textbooks
‚Ä¢ Let P denote the percept history and A denote the set of actions 
available. The agent policy or function is denoted by ùíá: ùë∑‚Üíùë®
‚Ä¢ Ideal mapping or expected behavior (‚Äúwhat it should do‚Äù)
‚Ä¢ Agent Program: A concrete implementation of the agent function 
on a machine. 
‚Ä¢ How the agent function is realized in practice. Example: A Python 
implementation 
6

Example: Vacuum Cleaner Agent
Sensors: Camera to detect 
obstacles and dirt
Percept Sequence: Cleanliness 
status
7

Example: Vacuum Cleaner Agent
Percept Sequence
Action
[A, Clean]
Right
[A, Dirty]
Suck
[B, Clean]
Left
[B, Dirty]
Suck
[A, Clean],[A, Clean]
Right
[A, Clean],[A, Dirty]
Suck
:
:
[A, Clean], [A, Clean], [A, Clean]
Right
[A, Clean], [A, Clean], [A, Dirty]
Suck
:
:
Sensors: Camera to detect 
obstacles and dirt
Percept Sequence: Cleanliness 
status 
Actuators: Suction brush, 
wheels
Agent function 
8

Agent Design and Implementation
Before implementing an agent, we need to think carefully about:
1.
The environment in which it will operate
2.
What sensors are required or available?
3.
How will it achieve the task? What capabilities are available?
‚Ä¢
A vacuum cleaner cannot clean the floor if does not have a 
mechanism to suck the dirt
4.
How should the agent act? How will its behavior or performance 
be evaluated?
9

PEAS Descriptions of Task Environments
Performance, Environment, Actuators, Sensors
Performance 
Measure
Environment
Actuators
Sensors
Safe, fast, 
legal, 
comfortable 
trip, maximize 
profits
Roads, other 
traffic, 
pedestrians, 
customers
Steering, 
accelerator, 
brake, signal, 
horn, display
Cameras, sonar, 
speedometer, 
GPS, odometer, 
accelerometer, 
engine sensors, 
keyboard
Example: Autonomous taxi
PEAS is the standard way of specifying an agent task environment before you 
even design the agent
10

PEAS Descriptions of Task Environments
Performance, Environment, Actuators, Sensors
Performance 
Measure
Environment
Actuators
Sensors
Healthy 
patient, 
minimize 
costs, lawsuits
Patient, hospital, 
staff
Display 
questions, tests, 
diagnoses, 
treatments, 
referrals
Keyboard entry 
of symptoms, 
findings, 
patient‚Äôs 
answers 
Example: Medical Diagnosis Agent
11

Properties of Environments
Fully observable: can access complete state 
of environment at each point in time
Partially observable: could be due to noisy, 
inaccurate or incomplete sensor data
Deterministic: next state of the 
environment completely determined by 
current state and agent‚Äôs action
Stochastic: when actions have multiple 
outcomes, each prescribed by a probability 
Episodic: agent‚Äôs experience divided into 
independent, atomic episodes in which 
agent perceives and performs a single 
action in each episode.
Sequential: current decision affects all future 
decisions
Static: agent doesn‚Äôt need to keep sensing 
while decides what action to take, doesn‚Äôt 
need to worry about time
Dynamic: environment changes while agent 
is thinking (changes with time)
Discrete: (note: applies to states, time, 
percepts, or actions)
Continuous: continuous values of states 
and/or actions
Single agent: single decision-making and 
executing entity 
Multiagent: multiple decision-
making/executing entities; cooperative or 
competitive
12

How Should An Agent Act?
Given:
‚Ä¢ A performance measure
‚Ä¢ A percept sequence
‚Ä¢ Agent‚Äôs knowledge
‚Ä¢ A set of available actions 
We still have one key piece before we can implement the agent‚Ä¶
How should an agent act? In other words, how should it select its 
actions? How should the agent function be constructed? 
Intelligence emerges from how agents choose actions
13

How Should An Agent Act?
Given:
‚Ä¢ A performance measure
‚Ä¢ A percept sequence
‚Ä¢ Agent‚Äôs knowledge
‚Ä¢ A set of available actions 
An ideal agent will outperform any other agent in maximizing 
the performance measure
Is this well-defined?
Does not consider the agent‚Äôs knowledge, its percept 
sequence or the environment in which it is operating
14

Rational Agent
Rational agent: for each possible percept sequence, a rational 
agent should select an action that is expected to maximize its 
performance measure, given the evidence provided by the 
percept sequence and whatever built-in knowledge the agent has
Rationality depends on 4 things:
1.
Performance measure of success
2.
Agent‚Äôs prior knowledge of environment
3.
Actions agent can perform
4.
Agent‚Äôs percept sequence to date
15

Types of Agents
‚Ä¢ Categorized based on their performance measure 
Types of 
Agents
Simple reflux 
agents
Model-based 
agents
Goal-directed 
agents
Utility-directed 
agents
16

Simple Reflex Agents
17
‚Ä¢ Simple if-then:  if condition then action; no internal model
‚Ä¢ Selects actions using only the current percept
function SIMPLE-REFLEX-AGENT(percept) returns an action
static: rules, a set of condition-action rules
state ‚Üê INTERPRET-INPUT(percept)
rule ‚Üê RULE-MATCH(state, rules)
action ‚Üê RULE-ACTION[rule]
return action
Example:   if temp < 68 ‚Üí turn heater on

Simple Reflex Agents
18

Simple Reflex Agents
‚Ä¢ Advantages:
‚Ä¢ Easy to implement
‚Ä¢ Uses much less memory than the table-driven agent
‚Ä¢ Disadvantages:
‚Ä¢ Will only work correctly if the environment is fully 
observable
‚Ä¢ Infinite loops
19

Model-based Reflex Agents
‚Ä¢ Maintain some internal state that keeps track of the part of the world 
it can‚Äôt see now (partial observability)
‚Ä¢ Needs model (encodes knowledge about how the world works)
function REFLEX-AGENT-WITH-STATE(percept) returns an action
static: state, a description of the current world state
rules, a set of condition-action rules
action, the most recent action, initially none
state ‚Üê UPDATE-STATE(state, action, percept)
rule ‚Üê RULE-MATCH(state, rules)
action ‚Üê RULE-ACTION[rule]
return action
Example: Vacuum cleaner robot
The internal model (‚Äúmap of which rooms are clean/dirty‚Äù) allows it to act 
sensibly despite partial observability.
20

Model-based Reflex Agents
21

Goal-directed Agents
‚Ä¢ Goal information guides agent‚Äôs actions (looks to the future)
‚Ä¢ Sometimes achieving goal is simple e.g. from a single action
‚Ä¢ Other times, goal requires reasoning about long sequences of 
actions √† accounts for future states
‚Ä¢ Flexible: simply reprogram the agent by changing goals
Example: Navigation 
22

Goal-directed Agents
23

Utility-directed Agents
‚Ä¢ What if there are many paths to the goal? √† optimize trade-offs 
when multiple outcomes are possible
‚Ä¢ Utility measures which states are preferable to other states
‚Ä¢ Assign numeric values to each possible outcome (utility or 
‚Äúhappiness‚Äù)
Common types of utility definitions:
‚Ä¢ Multidimensional utility (quality, failure rate, etc.)
‚Ä¢ Time-dependent utility (hard/soft deadlines)
‚Ä¢ Subjective vs. objective utility functions
‚Ä¢ Qualitative vs. quantitative
Example: Self-driving car balancing speed vs. safety vs. comfort
24

Utility-directed Agents
25

Types of Agents
‚Ä¢ Categorized based on their performance measure 
‚Ä¢ Simple reflux agents
‚Ä¢ Model-based reflex agents
‚Ä¢ Goal-directed agents
‚Ä¢ Utility-directed agents
‚Ä¢ Learning agents
26

Learning Agents
Successful agents split task of computing policy in 3 periods:
1.
Initially, designers compute some prior knowledge to include 
in policy
2.
When deciding its next action, agent does some computation
3.
Agent learns from experience to modify its behavior
Learn from experience to compensate for 
partial or incorrect prior knowledge
Example: Spam filter adapting over time. Initially misclassifies 
some emails, but improves by learning from user corrections 
(‚Äúmark as spam‚Äù / ‚Äúnot spam‚Äù)
27

Learning Agents
28

Learning Agents
Think of this as outside 
the agent since you don‚Äôt 
want it to be changed by 
the agent
Maps percepts to actions
29

Learning Agents
Responsible for improving the 
agent‚Äôs behavior with experience
Suggest actions to come 
up with new and 
informative experiences
Critic: Tells learning element how well 
the agent is doing with respect ot the 
performance standard (because the 
percepts don‚Äôt tell the agent about its 
success/failure)
30

In-Class Exercise: Designing Agents 
Design an intelligent vacuum cleaner for a house. 
To make it more interesting, we will analyze design choices for different 
house settings that will challenge us to think about what type of agent 
design works best in each setting.
Scenario 1: Small, single-room apartment
Simple reflex agent may be sufficient: If sensor = dirt ‚Üí suck; else ‚Üí move.
Scenario 2: Multi-room house
At least need a model-based agent, if not more complex designs: must 
remember which rooms have been cleaned
31

In-Class Exercise: Designing Agents 
Design an intelligent vacuum cleaner for a house. 
Scenario 3: Multi-room house with user-specific requests such as ‚ÄùClean the 
kitchen first and then the living room‚Äù
Goal-based agents 
32

In-Class Exercise: Designing Agents 
Design an intelligent vacuum cleaner for a house. 
Scenario 4: Multi-room house and the agent must determine the order of 
cleaning the rooms
1. Utility-based agents, because it must find the order in which the rooms 
must be cleaned such that the utility is maximized 
2. Learning agent that learns the dirt distribution (and therefore the utility 
associated with each room)
33

Designing Modern Agents
34
Example:
1. ChatGPT or other LLMs: utility-based, with learned utilities 
(preferences)
2. Robots in warehouses that perform pickup-and-place tasks: model-
based (uses an internal model of the environment) + learning 
(preferences, utilities, safety constraints)
Many of the modern AI agents are designed to learn from 
data, to adapt better to the environment and user 

From Classical AI to Modern Agents
35
Agent Type
Modern AI Example
Connection
Simple reflex agent
Basic rule-based systems such 
as thermostat control
Direct condition‚Äìaction 
mapping, no memory
Model-based agent
Roomba with mapping, 
warehouse robots
Maintains internal state of 
environment to act under 
partial observability
Goal-based agent
Delivery drones, planning in 
robotics, navigation apps 
(Google Maps, Waze)
Chooses actions to achieve 
explicit goals
Utility-based agent
Self-driving cars, 
recommendation systems, 
medical diagnosis AIs
Balances multiple objectives 
(e.g., safety vs. speed, accuracy 
vs. coverage)
Learning agent
Spam filters, ChatGPT, 
AlphaGo, adaptive robots
Improves behavior over time 
from data and feedback

Planning Agents
‚Ä¢ Need to plan a sequence of actions to reach the goal or complete 
a task 
‚Ä¢ Except simple reflex agents, all other types of agents perform 
some level of planning
‚Ä¢ Planning can be framed as a search over possible action 
sequences
36

Search Problem
37
‚Ä¢ Perform a search in the solution space intelligently
‚Ä¢ Evaluate solutions to identify the best
Solution space: set of all possible solutions to the problem
Solution

Formalizing the Search Problem
‚Ä¢ A finite set of states √† set of all decision points
‚Ä¢ Initial state
‚Ä¢ Goal test
‚Ä¢ Successor function: A successor function succ(s) which takes 
a state s as input and returns as output the set of states you 
can reach from state s in one step
‚Ä¢ Path cost: A cost function cost(s,s‚Äô) which returns the non-
negative one-step cost of travelling from state s to s‚Äô.  The 
cost function is only defined if s‚Äô is a successor state of s.
38

Formalizing the Search Problem: Example from 
Textbook
39
start
goal
